{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63135ec9-017e-4d6d-bfb2-2eff21f1538a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Step 1 - Data Generation\n",
    "\n",
    "Run the Setup notebook to generate synthetic data if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dc693bc-a835-4d20-95ad-774f0837b951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 2 - Data Governance\n",
    "\n",
    "### Cells 3-8 show how to mask sensitive fields so that only users in certain user groups will be able to see the information.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/main/images/hls/patient-readmission/hls-patient-readmision-flow-2.png\" style=\"float: right; margin-left: 10px; margin-top:10px\" width=\"650px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b79504b-42c5-4f53-8468-d3190cdea615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create widgets that will set the values for catalog name, schema name, and volume name:\n",
    "\n",
    "dbutils.widgets.text(name = \"source_catalog_name\", defaultValue=\"\", label=\"1. Source Catalog Name\")\n",
    "dbutils.widgets.text(name = \"source_schema_name\", defaultValue=\"\", label=\"2. Source Schema Name\")\n",
    "dbutils.widgets.text(name = \"destination_catalog_name\", defaultValue=\"\", label=\"3. Destination Catalog Name\")\n",
    "dbutils.widgets.text(name = \"destination_schema_name\", defaultValue=\"\", label=\"4. Destination Schema Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3974f77-d1d6-422a-8924-dc05aec3144b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_catalog_name = dbutils.widgets.get(\"source_catalog_name\")\n",
    "source_schema_name = dbutils.widgets.get(\"source_schema_name\")\n",
    "destination_catalog_name = dbutils.widgets.get(\"destination_catalog_name\")\n",
    "destination_schema_name = dbutils.widgets.get(\"destination_schema_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b94c6ba5-e5f1-41f8-9899-62fdd32e8e74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f'CREATE CATALOG IF NOT EXISTS {catalog_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5e9f084-a20d-41e6-990b-3d93984ba54f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG IDENTIFIER(:source_catalog_name);\n",
    "USE SCHEMA IDENTIFIER(:source_schema_name);\n",
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f23ecbf-1b72-4a55-92db-d226851bf420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Let's grant our ANALYSTS a SELECT permission:\n",
    "-- Note: make sure you created an analysts and dataengineers group first.\n",
    "GRANT SELECT ON TABLE patients TO `analysts`;\n",
    "-- GRANT SELECT ON TABLE condition_occurrence TO `analysts`;\n",
    "-- GRANT SELECT ON TABLE encounters TO `analysts`;\n",
    "\n",
    "-- We'll grant an extra MODIFY to our Data Engineer\n",
    "-- GRANT SELECT, MODIFY ON SCHEMA dbdemos_hls_readmission TO `dataengineers`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507e44d2-33b0-47e8-bef7-c474f8d09748",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE OR REPLACE TABLE {destination_catalog_name}.{destination_schema_name}.protected_patients AS SELECT * FROM patients;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c17e9781-4b4e-4cbe-a67f-5ac37a32bf15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- hls_admin group will have access to all data, all other users will see masks over the sensitive fields.\n",
    "CREATE OR REPLACE FUNCTION simple_mask(column_value STRING)\n",
    "   RETURN IF(is_account_group_member('hls_admin'), column_value, \"****\");\n",
    "   \n",
    "-- ALTER FUNCTION simple_mask OWNER TO `account users`; -- grant access to all user to the function for the demo - don't do it in production\n",
    "\n",
    "-- Mask all PII information\n",
    "ALTER TABLE protected_patients ALTER COLUMN FIRST SET MASK simple_mask;\n",
    "ALTER TABLE protected_patients ALTER COLUMN LAST SET MASK simple_mask;\n",
    "ALTER TABLE protected_patients ALTER COLUMN PASSPORT SET MASK simple_mask;\n",
    "ALTER TABLE protected_patients ALTER COLUMN DRIVERS SET MASK simple_mask;\n",
    "ALTER TABLE protected_patients ALTER COLUMN SSN SET MASK simple_mask;\n",
    "ALTER TABLE protected_patients ALTER COLUMN ADDRESS SET MASK simple_mask;\n",
    "\n",
    "SELECT * FROM protected_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a33cf3c5-98e1-4c60-a2f2-e39f0c4b2953",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# CREATE SHARE IF NOT EXISTS mcutini_diabetes_readmissions_share\n",
    "#   COMMENT 'Share the patients table in mcutini.arizona_patients for diabetes research.';\n",
    "\n",
    "# ALTER SHARE mcutini_diabetes_readmissions_share OWNER TO `HOOLI`;\n",
    "\n",
    "# ALTER SHARE mcutini_diabetes_readmissions_share ADD TABLE patients WITH HISTORY;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d88a6409-5076-48c0-9b46-94282b2feb12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# DESCRIBE SHARE mcutini_diabetes_readmissions_share;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7326cd75-1d71-4638-a91f-34a0e7250f06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 3 - Data Analysis\n",
    "\n",
    "### Cells 10-35 show the experience of analyzing your data using Python and SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32bb7610-bfa3-4cfa-bab6-7a832e9d06ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "select * from patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df199ad6-ab52-44a0-8230-ef4d3d16caf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select GENDER, ceil(months_between(current_date(),BIRTHDATE)/12/5)*5 as age, count(*) as count from patients group by GENDER, age order by age\n",
    "-- Can use buildin visualization (Area: Key: age, group: gender_source_value, Values: count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05883400-f46b-4043-ac42-fded8cca6334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "px.area(_sqldf.toPandas(), x=\"age\", y=\"count\", color=\"GENDER\", line_group=\"GENDER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e282ce5-ae36-4a4d-a4aa-988157761c6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We can also leverage pure Python to access data\n",
    "from pyspark.sql.functions import col, desc\n",
    "df = spark.table(\"patients\").join(spark.table(\"conditions\"), col(\"Id\")==col(\"PATIENT\")) \\\n",
    "          .groupBy(['GENDER', 'conditions.DESCRIPTION']).count() \\\n",
    "          .orderBy(desc('count')).limit(20).toPandas()\n",
    "#And use our usual plot libraries\n",
    "px.bar(df, x=\"DESCRIPTION\", y=\"count\", color=\"GENDER\", barmode=\"group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db78218d-8588-4593-ab05-ba029447f79b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cohort Definition\n",
    "\n",
    "Let's define a cohort that we can do analysis on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ebae259-855c-41e6-a639-17ef2a7d7235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE cohort (\n",
    "  id INT,\n",
    "  name STRING,\n",
    "  patient STRING,\n",
    "  cohort_start_date DATE,\n",
    "  cohort_end_date DATE\n",
    ");\n",
    "ALTER TABLE cohort OWNER TO `account users`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0823d19-7d5b-4800-a4d0-75f8f75d8991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.sql.functions import lit\n",
    "def create_save_cohort(name, condition_codes = []):\n",
    "  cohort1 = (spark.sql('select patient, to_date(start) as cohort_start_date, to_date(stop) as cohort_end_date from conditions')\n",
    "                 .withColumn('id', lit(random.randint(999999, 99999999)))\n",
    "                 .withColumn('name', lit(name)))\n",
    "  if len(condition_codes)> 0:\n",
    "    cohort1 = cohort1.where(col('CODE').isin(condition_codes))\n",
    "  cohort1.write.mode(\"append\").saveAsTable('cohort')\n",
    "\n",
    "#Create cohorts based on patient condition (for ex: 840539006 is COVID)\n",
    "create_save_cohort('COVID-19-cohort', [840539006])\n",
    "create_save_cohort('heart-condition-cohort', [1505002, 32485007, 305351004, 76464004])\n",
    "create_save_cohort('all_patients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94e435b7-d563-4e9e-9050-a5221495f6bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 4 - Data Science and Machine Learning\n",
    "\n",
    "### Cells 38-50 show how do to data science on your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2376d533-cd05-456f-89ef-c39171bd9c7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/main/images/hls/patient-readmission/hls-patient-readmision-flow-4.png\" style=\"float: right; margin-left: 30px; margin-top:10px\" width=\"650px\" />\n",
    "\n",
    "We have cleaned and secured our data. We have also created cohorts of patients to analyze. We can now predict 30-day readmissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "796cab6a-43f0-4b2e-8bfa-b16aa5dae6db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow==3.1.1 #2.19.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a6b128f-76bd-4d73-a3e3-340c2ed6c62a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80046596-eadf-4752-8af3-b4e2d2456167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBMZXQncyBjcmVhdGUgb3VyIGxhYmVsOiB3ZSdsbCBwcmVkaWN0IHRoZSAgMzAgZGF5cyByZWFkbWlzc2lvbiByaXNrCmZyb20gcHlzcGFyay5zcWwgaW1wb3J0IFdpbmRvdwpmcm9tIHB5c3Bhcmsuc3FsLmZ1bmN0aW9ucyBpbXBvcnQgdG9fdGltZXN0YW1wCndpbmRvd1NwZWMgPSBXaW5kb3cucGFydGl0aW9uQnkoIlBBVElFTlQiKS5vcmRlckJ5KCJTVEFSVCIpCmxhYmVscyA9IHNwYXJrLnRhYmxlKCdlbmNvdW50ZXJzJykuc2VsZWN0KCJQQVRJRU5UIiwgIklkIiwgIlNUQVJUIiwgIlNUT1AiKSBcCiAgICAgICAgICAgICAgLndpdGhDb2x1bW4oJzMwX0RBWV9SRUFETUlTU0lPTicsIEYud2hlbih0b190aW1lc3RhbXAoRi5jb2woJ1NUQVJUJykpLmNhc3QoJ2xvbmcnKSAtIEYubGFnKHRvX3RpbWVzdGFtcChGLmNvbCgnU1RPUCcpKSkub3Zlcih3aW5kb3dTcGVjKS5jYXN0KCdsb25nJykgPCAzMCoyNCo2MCo2MCwgMSkub3RoZXJ3aXNlKDApKQpkaXNwbGF5KGxhYmVscyk=\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1752021171076,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "9847aa96-ec56-40f5-8672-cf5aca6027c6",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 40.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1752021171076,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 1752021074329,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's create our label: we'll predict the  30 days readmission risk\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "windowSpec = Window.partitionBy(\"PATIENT\").orderBy(\"START\")\n",
    "labels = spark.table('encounters').select(\"PATIENT\", \"Id\", \"START\", \"STOP\") \\\n",
    "              .withColumn('30_DAY_READMISSION', F.when(to_timestamp(F.col('START')).cast('long') - F.lag(to_timestamp(F.col('STOP'))).over(windowSpec).cast('long') < 30*24*60*60, 1).otherwise(0))\n",
    "display(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b32bfe3f-6438-466f-8dc8-1f39a9b6fd29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "# Define Patient Features logic\n",
    "def compute_pat_features(data):\n",
    "  data = data.pandas_api()\n",
    "  data = ps.get_dummies(data, columns=['MARITAL', 'RACE', 'ETHNICITY', 'GENDER'],dtype = 'int64').to_spark()\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c23bdf78-ce25-4f7c-9cb8-65eb6f14e459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cohort_name = 'all_patients' #or could be 'COVID-19-cohort'\n",
    "cohort = spark.sql(f\"SELECT p.* FROM cohort c INNER JOIN patients p on c.patient=p.id WHERE c.name='{cohort_name}'\").dropDuplicates([\"id\"])\n",
    "cohort_features_df = compute_pat_features(cohort)\n",
    "cohort_features_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa43bce-fa00-4502-8472-0e57963892e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def compute_enc_features(data):\n",
    "  data = data.dropDuplicates([\"Id\"])\n",
    "  data = data.withColumn('enc_length', F.unix_timestamp(col('stop'))- F.unix_timestamp(col('start')))\n",
    "  data = data.pandas_api()\n",
    "#   return data\n",
    "  data = ps.get_dummies(data, columns=['ENCOUNTERCLASS'],dtype = 'int64').to_spark()\n",
    "  \n",
    "  return (\n",
    "    data\n",
    "    .select(\n",
    "      col('Id').alias('ENCOUNTER_ID'),\n",
    "      'BASE_ENCOUNTER_COST',\n",
    "      'TOTAL_CLAIM_COST',\n",
    "      'PAYER_COVERAGE',\n",
    "      'enc_length',\n",
    "      'ENCOUNTERCLASS_ambulatory',\n",
    "      'ENCOUNTERCLASS_emergency',\n",
    "      'ENCOUNTERCLASS_inpatient',\n",
    "      'ENCOUNTERCLASS_outpatient',\n",
    "      'ENCOUNTERCLASS_wellness',\n",
    "    )\n",
    "  )\n",
    "\n",
    "enc_features_df = compute_enc_features(spark.table('encounters'))\n",
    "display(enc_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fb2e23a-7471-4179-8a77-a2501f7abde7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "enc_features_df = compute_enc_features(spark.table('encounters'))\n",
    "training_dataset = cohort_features_df.join(labels, [labels.PATIENT==cohort_features_df.Id], \"inner\") \\\n",
    "                                     .join(enc_features_df, [labels.Id==enc_features_df.ENCOUNTER_ID], \"inner\") \\\n",
    "                                     .drop(\"Id\", \"_rescued_data\", \"SSN\", \"DRIVERS\", \"PASSPORT\", \"FIRST\", \"LAST\", \"ADDRESS\", \"BIRTHPLACE\")\n",
    "### Adding extra feature such as patient age at encounter\n",
    "training_dataset = training_dataset.withColumnRenamed(\"PATIENT\", \"patient_id\") \\\n",
    "                                   .withColumn(\"age_at_encounter\", ((F.datediff(col('START'), col('BIRTHDATE'))) / 365.25))\n",
    "\n",
    "training_dataset.write.mode('overwrite').saveAsTable(\"training_dataset\")\n",
    "display(spark.table(\"training_dataset\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ff07d4f-fd29-4db8-afe4-b8ffe86b3c0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89f2b4ed-873a-4ea0-97a6-69b382d8143d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_names = ['MARITAL_M', 'MARITAL_S', 'RACE_asian', 'RACE_black', 'RACE_white', 'ETHNICITY_hispanic', 'ETHNICITY_nonhispanic', 'GENDER_F', 'GENDER_M', 'INCOME'] \\\n",
    "              + ['BASE_ENCOUNTER_COST', 'TOTAL_CLAIM_COST', 'PAYER_COVERAGE', 'enc_length', 'ENCOUNTERCLASS_ambulatory', 'ENCOUNTERCLASS_emergency', 'ENCOUNTERCLASS_inpatient', 'ENCOUNTERCLASS_outpatient', 'ENCOUNTERCLASS_wellness'] \\\n",
    "              + ['age_at_encounter'] \\\n",
    "              + ['30_DAY_READMISSION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe2d6a67-4b07-4789-bb77-75704f163e55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def set_experiment_permission(experiment_path):\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    from databricks.sdk.service import iam\n",
    "    w = WorkspaceClient()\n",
    "    try:\n",
    "      status = w.workspace.get_status(experiment_path)\n",
    "      w.permissions.set(\"experiments\", request_object_id=status.object_id,  access_control_list=[\n",
    "                            iam.AccessControlRequest(group_name=\"users\", permission_level=iam.PermissionLevel.CAN_MANAGE)])    \n",
    "    except Exception as e:\n",
    "      print(f\"error setting up shared experiment {experiment_path} permission: {e}\")\n",
    "\n",
    "    print(f\"Experiment on {experiment_path} was set public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9222038f-d144-4179-830f-a10f1177b962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from datetime import datetime\n",
    "model_name = f\"{catalog_name}_diabetes_readmissions\"\n",
    "xp_path = \"/Workspace/Users/matt.cutini@databricks.com/diabetes_readmission\"\n",
    "xp_name = f\"automl_churn_{datetime.now().strftime('%Y-%m-%d_%H:%M:%S')}\"\n",
    "try:\n",
    "    from databricks import automl\n",
    "    automl_run = automl.classify(\n",
    "        experiment_name = xp_name,\n",
    "        experiment_dir = xp_path,\n",
    "        dataset = training_dataset.select(feature_names),\n",
    "        target_col = \"30_DAY_READMISSION\",\n",
    "        primary_metric=\"roc_auc\",\n",
    "        timeout_minutes = 10\n",
    "    )\n",
    "    #Make sure all users can access dbdemos shared experiment\n",
    "    set_experiment_permission(f\"{xp_path}/{xp_name}\")\n",
    "except Exception as e:\n",
    "    if \"cannot import name 'automl'\" in str(e):\n",
    "        # Note: cannot import name 'automl' from 'databricks' likely means you're using serverless. Dbdemos doesn't support autoML serverless API - this will be improved soon.\n",
    "        # Adding a temporary workaround to make sure it works well for now - ignore this for classic run\n",
    "        automl_run = DBDemos.create_mockup_automl_run(f\"{xp_path}/{xp_name}\", training_dataset.select(feature_names).toPandas(), model_name = model_name, target_col = \"30_DAY_READMISSION\")\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15e4387b-45b0-4400-88c3-50465f49d6f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Enable Unity Catalog with mlflow registry\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "    \n",
    "model_registered = mlflow.register_model(f\"runs:/{automl_run.best_trial.mlflow_run_id}/model\", f\"{catalog_name}.{schema_name}.{model_name}\")\n",
    "\n",
    "#Move the model in production\n",
    "print(\"registering model version \"+model_registered.version+\" as production model\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "client.set_registered_model_alias(name=f\"{catalog_name}.{schema_name}.{model_name}\", alias=\"prod\", version=model_registered.version)\n",
    "\n",
    "#Make sure all other users can access the model for our demo\n",
    "#DBDemos.set_model_permission(f\"{catalog}.{db}.{model_name}\", \"ALL_PRIVILEGES\", \"account users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f3938a4-b2a6-484a-ae53-2b98583a95dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Batch Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a963be-ef33-41ac-9115-1882d7c36abc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load model as a Spark UDF.\n",
    "loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=f\"models:/{catalog_name}.{schema_name}.{model_name}@prod\", result_type='double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a49fca54-48b6-4faa-b2eb-18823d755b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "features = loaded_model.metadata.get_input_schema().input_names()\n",
    "\n",
    "#For this demo, reuse our dataset to test the batch inferences\n",
    "test_dataset = spark.table('training_dataset')\n",
    "\n",
    "patient_risk_df =  test_dataset \\\n",
    "                   .withColumn(\"risk_prediction\", loaded_model(F.struct(*features))) \\\n",
    "                   .select('ENCOUNTER_ID', 'PATIENT_ID', 'risk_prediction')\n",
    "\n",
    "display(patient_risk_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "135d8ec4-9046-437e-9aeb-1cc11ee6080b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "patient_risk_df.write.mode(\"overwrite\").saveAsTable(f\"patient_readmission_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "747c05d5-d0a7-4087-b80a-3b7ea3049f06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "154c9745-1245-4e5a-9d3a-3ed1107b5ef9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "full_model_name = f\"{catalog_name}.{schema_name}.{model_name}\"\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "#Enable Unity Catalog with mlflow registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = MlflowClient(registry_uri=\"databricks-uc\")\n",
    "\n",
    "#Get model with PROD alias (make sure you run the notebook 04.2 to save the model in UC)\n",
    "latest_model = client.get_model_version_by_alias(full_model_name, \"prod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04b36966-325d-4f71-b6d2-203acccc4e80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import ServedEntityInput, EndpointCoreConfigInput, AutoCaptureConfigInput\n",
    "\n",
    "serving_endpoint_name = \"mcutini_diabetes_readmissions_endpoint\"\n",
    "w = WorkspaceClient()\n",
    "\n",
    "endpoint_config = EndpointCoreConfigInput(\n",
    "    name=serving_endpoint_name,\n",
    "    served_entities=[\n",
    "        ServedEntityInput(\n",
    "            entity_name=full_model_name,\n",
    "            entity_version=latest_model.version,\n",
    "            scale_to_zero_enabled=True,\n",
    "            workload_size=\"Small\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "force_update = False #Set this to True to release a newer version (the demo won't update the endpoint to a newer model version by default)\n",
    "try:\n",
    "  existing_endpoint = w.serving_endpoints.get(serving_endpoint_name)\n",
    "  print(f\"endpoint {serving_endpoint_name} already exist - force update = {force_update}...\")\n",
    "  if force_update:\n",
    "    w.serving_endpoints.update_config_and_wait(served_entities=endpoint_config.served_entities, name=serving_endpoint_name)\n",
    "except:\n",
    "    print(f\"Creating the endpoint {serving_endpoint_name}, this will take a few minutes to package and deploy the endpoint...\")\n",
    "    w.serving_endpoints.create_and_wait(name=serving_endpoint_name, config=endpoint_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10136671-d7c7-424d-897a-7a2e8a0f4676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\n",
    "from mlflow.models.model import Model\n",
    "\n",
    "p = ModelsArtifactRepository(f\"models:/{full_model_name}@prod\").download_artifacts(\"\") \n",
    "dataset =  {\"dataframe_split\": Model.load(p).load_input_example(p).to_dict(orient='split')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25a1e841-0fe2-4091-b66b-2526bed00da8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import deployments\n",
    "deployment_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "predictions = deployment_client.predict(endpoint=serving_endpoint_name, inputs=dataset)\n",
    "\n",
    "print(f\"Patient readmission risk: {predictions}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "433450cf-b3e4-4992-85fe-a6d91be31729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "165d5b1f-0d72-4ddd-840b-1f2953a73a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#For this demo, reuse our dataset to test the batch inferences\n",
    "dataset_to_explain = spark.table('training_dataset')\n",
    "dataset_to_explain.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f92bccd-1960-46ec-8ba9-36ebeb917e51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "#Enable Unity Catalog with mlflow registry\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{catalog_name}.{schema_name}.{model_name}@prod\")\n",
    "features = model.metadata.get_input_schema().input_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba13da9-2e41-45f2-8cc1-2f183ce71627",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "mlflow.autolog(disable=True)\n",
    "mlflow.sklearn.autolog(disable=True)\n",
    "\n",
    "df = dataset_to_explain.sample(fraction=0.1).toPandas()\n",
    "\n",
    "train_sample = df[features].sample(n=np.minimum(100, df.shape[0]), random_state=42)\n",
    "\n",
    "# Use Kernel SHAP to explain feature importance on the sampled rows from the validation set.\n",
    "predict = lambda x: model.predict(pd.DataFrame(x, columns=features).astype(train_sample.dtypes.to_dict()))\n",
    "\n",
    "explainer = shap.KernelExplainer(predict, train_sample, link=\"identity\")\n",
    "shap_values = explainer.shap_values(train_sample, l1_reg=False, nsamples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "752cfbbf-1d04-4e5a-9877-fd09dad96251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "mean_abs_shap = np.absolute(shap_values).mean(axis=0).tolist()\n",
    "df = pd.DataFrame(list(zip(mean_abs_shap,features)), columns=['SHAP_value', 'feature'])\n",
    "px.bar(df.sort_values('SHAP_value', ascending=False).head(10), x='feature', y='SHAP_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d03a21-aff6-4b6d-856b-1fcc0e9b6f29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a91ff18-3c88-4a2e-92a6-0b7618275521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We'll need to add shap bundle js to display nice graph\n",
    "with open(shap.__file__[:shap.__file__.rfind('/')]+\"/plots/resources/bundle.js\", 'r') as file:\n",
    "   shap_bundle_js = '<script type=\"text/javascript\">'+file.read()+'</script>'\n",
    "\n",
    "html = shap.force_plot(explainer.expected_value, shap_values[0,:], train_sample.iloc[0,:])\n",
    "displayHTML(shap_bundle_js + html.html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "066eafe7-ac0e-481c-a979-5742f076e6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_html = shap.force_plot(explainer.expected_value, shap_values, train_sample)\n",
    "displayHTML(shap_bundle_js + plot_html.html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73852382-38b3-4869-9662-d2e80c961cc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"INCOME\", shap_values, train_sample[features], interaction_index=\"TOTAL_CLAIM_COST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fc3a6a4-6554-4d10-8c8d-39c847ba0330",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def compute_shap_values(iterator):\n",
    "  for X in iterator:\n",
    "    yield pd.DataFrame(explainer.shap_values(X, check_additivity=False))\n",
    "\n",
    "df = dataset_to_explain.mapInPandas(compute_shap_values, schema=\", \".join([x+\"_shap_value float\" for x in features]))\n",
    "\n",
    "# Skip as this can take some time to run\n",
    "#display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3878688999370385,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Diabetes Readmissions Demo",
   "widgets": {
    "destination_catalog_name": {
     "currentValue": "mcutini",
     "nuid": "2d3258e6-4921-4baf-aa09-dc659f4d47f4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "3. Destination Catalog Name",
      "name": "destination_catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "3. Destination Catalog Name",
      "name": "destination_catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "destination_schema_name": {
     "currentValue": "utsw_test",
     "nuid": "82e1b4c8-f8af-44f0-bc8b-704bae926556",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "4. Destination Schema Name",
      "name": "destination_schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "4. Destination Schema Name",
      "name": "destination_schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "source_catalog_name": {
     "currentValue": "mcutini",
     "nuid": "b58fa697-0f4c-438e-a333-03090518ea05",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "1. Source Catalog Name",
      "name": "source_catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "1. Source Catalog Name",
      "name": "source_catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "source_schema_name": {
     "currentValue": "arizona_patients",
     "nuid": "0ee258f2-e0cf-41b7-80fc-ed3a814620c2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "2. Source Schema Name",
      "name": "source_schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "2. Source Schema Name",
      "name": "source_schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
