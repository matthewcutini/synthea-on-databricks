{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63135ec9-017e-4d6d-bfb2-2eff21f1538a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Step 1 - Data Generation\n",
    "\n",
    "### Cells 2-20 run Synthea synthetic data generator. Skip these cells if you already have data to work with.\n",
    "\n",
    "Synthea runs on Java Development Kit (JDK) 17, so use a cluster that has DBR 16.0 or above, as JDK 17 is the default. Check that you have JDK 17 installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24f6dffb-e9de-468e-bcf8-9a2c70b541e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bed01633-5fee-4f96-b00e-4b9d9cb787d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(name = \"catalog_name\", defaultValue=\"\", label=\"Catalog Name\")\n",
    "dbutils.widgets.text(name = \"schema_name\", defaultValue=\"\", label=\"Schema Name\")\n",
    "dbutils.widgets.text(name = \"destination\", defaultValue=\"./output/\", label = \"Base Directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbb42946-c7dd-4efd-888f-1c9781f7602f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = dbutils.widgets.get(name = \"catalog_name\")\n",
    "schema_name = dbutils.widgets.get(name = \"schema_name\")\n",
    "destination = dbutils.widgets.get(name = \"destination\")\n",
    "volume_path = f\"/Volumes/{catalog_name}/{schema_name}/synthetic_files_raw/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b37bdadd-3bc6-400b-87fc-a4da0ca9a279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Code that may raise an exception\n",
    "    dbutils.fs.ls(f\"{volume_path}synthea_config.txt\")\n",
    "    result = \"True\"  # Return 0 if it works\n",
    "except:\n",
    "    result = \"False\"  # Return 1 if an exception occurs\n",
    "\n",
    "result  # Return the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1be00f50-1b16-4526-8498-dba482191299",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.jobs.taskValues.set(key = 'result', value = result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff31eb5b-ec2f-4e70-9eaa-cbeb45f11c6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS IDENTIFIER(:catalog_name);\n",
    "USE CATALOG IDENTIFIER(:catalog_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80639340-c582-42aa-8646-ed5eb4d81aae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create schema if not exists IDENTIFIER(:schema_name);\n",
    "use schema IDENTIFIER(:schema_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1dffdeb-9682-4f60-89c9-119c845bbcad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create volume if not exists synthetic_files_raw;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa5ef5cc-53ed-4c6c-be61-3b389e5803e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the latest Synthea release\n",
    "from urllib.request import urlretrieve\n",
    "urlretrieve(\n",
    "  url = \"https://github.com/synthetichealth/synthea/releases/download/master-branch-latest/synthea-with-dependencies.jar\"\n",
    "  ,filename = f\"{volume_path}synthea-with-dependencies.jar\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b21222d-6cc3-4ced-85ae-a74c28ed62a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Execute the Synthea JAR one time to initialize\n",
    "command = f\"\"\"\n",
    "cd {volume_path}\n",
    "java -jar synthea-with-dependencies.jar\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e9c0726-7e91-418a-a108-87c9a6d9906d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a Synthea configuration file and write it to the volume\n",
    "config_file_text = (\n",
    "f\"\"\"# synthea streaming simulation configuration file\n",
    "exporter.ccda.export = false\n",
    "exporter.fhir.export = false\n",
    "exporter.csv.export = true\n",
    "exporter.csv.folder_per_run = true\n",
    "exporter.years_of_history = 5\n",
    "exporter.baseDirectory = {destination}\n",
    "generate.append_numbers_to_person_names = false\n",
    "generate.default_population = 100\n",
    "exporter.clinical_note.export = true\n",
    "generate.costs.default_encounter_cost = 1250.00\n",
    "\"\"\")\n",
    "\n",
    "filename = f\"{volume_path}synthea_config.txt\"\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(config_file_text)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb37a813-5662-4dc9-9a4b-f5de8ab00111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def data_generator(volume_path: str = volume_path, config_file_path: str = f\"{volume_path}synthea_config.txt\", additional_options: str = \"\", verbose: bool = False):\n",
    "  command = (\n",
    "  f\"\"\"cd {volume_path}\n",
    "  java -jar synthea-with-dependencies.jar -s 666 -cs 777 -r 20200626 -k keep.json -c {config_file_path} {additional_options}\n",
    "  \"\"\")\n",
    "  if verbose == True:\n",
    "    print(command)\n",
    "  result = subprocess.run([command], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, shell=True)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6ec18a9-a18e-400f-890f-ffa410723bd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "run_results = data_generator(\n",
    "\n",
    "   volume_path=volume_path\n",
    "  ,config_file_path=f\"{volume_path}synthea_config.txt\"\n",
    "  ,additional_options=\"Utah\"\n",
    "  ,verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aaf2961-9e0c-49c1-8221-6eb42aba1355",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_volume_path = f\"/Volumes/{catalog_name}/{schema_name}/synthetic_files_raw/\"\n",
    "target_volume_path = f\"/Volumes/{catalog_name}/{schema_name}/landing/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a66215e-2df0-4b97-be9a-1256cf66293b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create landing zone volume if not exists\n",
    "spark.sql(f'CREATE VOLUME IF NOT EXISTS {catalog_name}.{schema_name}.landing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a54c4051-da82-43be-bf23-787ebb19b863",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Copy new files from synthetic_files_raw/output/csv folder to landing zone\n",
    "\n",
    "import os\n",
    "\n",
    "# get directories and order by file name (timestamp) in ascending order (ensure correct processing order)\n",
    "directories = dbutils.fs.ls(f\"{source_volume_path}/output/csv\")\n",
    "directories_sorted = sorted(directories)\n",
    "\n",
    "# for each directory, get files and move them to landing\n",
    "for directory in directories_sorted:\n",
    "  file_path = directory[0]\n",
    "  directory = directory[1].split('/')[0]\n",
    "  files = spark.sql(f\"LIST '{file_path}' \")\n",
    "  # define file/directory to ignore\n",
    "  file_exception = 'data_quality_output_data_quality_output/'  \n",
    "  print(f\"Copying files from directory: {directory} \\n source:{file_path}  \\n target:{target_volume_path}\")\n",
    "\n",
    "  # get files in given directory\n",
    "  for file in files.collect():\n",
    "    # create a folder for the csv based off of file name\n",
    "    file_path = file[0]\n",
    "    file_time = file_path.split('/')[-2]\n",
    "    directory_name = file[1].split('.')[0]\n",
    "    file_name = file_time + '_' + file[1].split('.')[0]\n",
    "    \n",
    "    # check if file exists and copy file\n",
    "    dst = f\"{target_volume_path}{directory_name}/{file_name}.csv\"\n",
    "\n",
    "    if os.path.exists(dst):\n",
    "      print(f'File already exists, skipping file: {file_name}.csv')\n",
    "    else:\n",
    "      print(f'Copying file: {file_name}.csv to target: {target_volume_path}')\n",
    "      dbutils.fs.cp(f\"{file_path}\", dst)\n",
    "  print(f'Successfully copied files to target \\n target: {target_volume_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2d330c3-3e33-446c-b530-562e961acd77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Copy new files from synthetic_files_raw/output/notes to landing zone\n",
    "\n",
    "source_path = f\"/Volumes/{catalog_name}/{schema_name}/synthetic_files_raw/output/notes\"\n",
    "target_path = f\"/Volumes/{catalog_name}/{schema_name}/landing/notes/\"\n",
    "\n",
    "files = [file.path for file in dbutils.fs.ls(source_path)]\n",
    "\n",
    "for file in files:\n",
    "    file_name = file.split('/')[-1]\n",
    "    dst = f\"{target_path}{file_name}\"\n",
    "\n",
    "    if os.path.exists(dst):\n",
    "        print(f'File already exists, skipping file: {file_name}')\n",
    "    else:\n",
    "        print(f'Copying file: {file_name} to target: {target_path}')\n",
    "        dbutils.fs.cp(file, dst, recurse=True)\n",
    "\n",
    "print(f'Successfully copied files to target: {target_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dc5cde7-3f52-4ef9-a662-6b5bd253a0b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ingest the CSV files into Delta tables\n",
    "\n",
    "# Define the base path to the landing folder\n",
    "base_path = f\"/Volumes/{catalog_name}/{schema_name}/landing/\"\n",
    "\n",
    "# Get all subdirectories in the base path\n",
    "subdirectories = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "\n",
    "# Remove the 'notes' folder from the subdirectories list\n",
    "subdirectories = [d for d in subdirectories if d != 'notes']\n",
    "\n",
    "print(subdirectories)\n",
    "\n",
    "# Iterate over each subdirectory and process the files\n",
    "for subdir in subdirectories:\n",
    "    file_path = os.path.join(base_path, subdir)\n",
    "    \n",
    "    # Read the files into a DataFrame\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n",
    "    \n",
    "    # Write the DataFrame to a Delta table\n",
    "    table_name = f\"{catalog_name}.{schema_name}.{subdir}\"\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b45a11fc-f494-40ab-8558-c5ddb7219c47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ingest the notes files (in txt format) into Delta tables\n",
    "\n",
    "# Define the path to the notes folder\n",
    "notes_path = f\"/Volumes/{catalog_name}/{schema_name}/synthetic_files_raw/output/notes/\"\n",
    "\n",
    "# Get all files in the notes folder\n",
    "note_files = [f for f in os.listdir(notes_path) if os.path.isfile(os.path.join(notes_path, f))]\n",
    "\n",
    "# Create a list to hold the file data\n",
    "data = []\n",
    "\n",
    "# Iterate over each file and read its content\n",
    "for note_file in note_files:\n",
    "    file_path = os.path.join(notes_path, note_file)\n",
    "    with open(file_path, 'r') as file:\n",
    "        file_text = file.read()\n",
    "        data.append((note_file, file_text))\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "notes_df = spark.createDataFrame(data, [\"file_name\", \"file_text\"])\n",
    "\n",
    "# Write the DataFrame to a Delta table\n",
    "notes_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.notes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dc693bc-a835-4d20-95ad-774f0837b951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 2 - Data Governance\n",
    "\n",
    "### Cells 22-27 show how to mask sensitive fields so that only users in certain user groups will be able to see the information.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/main/images/hls/patient-readmission/hls-patient-readmision-flow-2.png\" style=\"float: right; margin-left: 10px; margin-top:10px\" width=\"650px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5e9f084-a20d-41e6-990b-3d93984ba54f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG ${catalog_name};\n",
    "USE SCHEMA ${schema_name};\n",
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f23ecbf-1b72-4a55-92db-d226851bf420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Let's grant our ANALYSTS a SELECT permission:\n",
    "-- Note: make sure you created an analysts and dataengineers group first.\n",
    "GRANT SELECT ON TABLE drug_exposure TO `analysts`;\n",
    "GRANT SELECT ON TABLE condition_occurrence TO `analysts`;\n",
    "GRANT SELECT ON TABLE patients TO `analysts`;\n",
    "\n",
    "-- We'll grant an extra MODIFY to our Data Engineer\n",
    "-- GRANT SELECT, MODIFY ON SCHEMA dbdemos_hls_readmission TO `dataengineers`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507e44d2-33b0-47e8-bef7-c474f8d09748",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE protected_patients AS SELECT * FROM patients;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c17e9781-4b4e-4cbe-a67f-5ac37a32bf15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- hls_admin group will have access to all data, all other users will see a masked information.\n",
    "CREATE OR REPLACE FUNCTION simple_mask(column_value STRING)\n",
    "   RETURN IF(is_account_group_member('hls_admin'), column_value, \"****\");\n",
    "   \n",
    "-- ALTER FUNCTION simple_mask OWNER TO `account users`; -- grant access to all user to the function for the demo - don't do it in production\n",
    "\n",
    "-- Mask all PII information\n",
    "ALTER TABLE protected_patients ALTER COLUMN FIRST SET MASK simple_mask;\n",
    "ALTER TABLE protected_patients ALTER COLUMN LAST SET MASK simple_mask;\n",
    "ALTER TABLE protected_patients ALTER COLUMN PASSPORT SET MASK simple_mask;\n",
    "ALTER TABLE protected_patients ALTER COLUMN DRIVERS SET MASK simple_mask;\n",
    "ALTER TABLE protected_patients ALTER COLUMN SSN SET MASK simple_mask;\n",
    "ALTER TABLE protected_patients ALTER COLUMN ADDRESS SET MASK simple_mask;\n",
    "\n",
    "SELECT * FROM protected_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a33cf3c5-98e1-4c60-a2f2-e39f0c4b2953",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SHARE IF NOT EXISTS mcutini_diabetes_readmissions_share\n",
    "  COMMENT 'Share the information in diabetes_readmissions.';\n",
    " \n",
    "-- For the demo we'll grant ownership to all users. Typical deployments wouls have admin groups or similar.\n",
    "ALTER SHARE mcutini_diabetes_readmissions_share OWNER TO `account users`;\n",
    "\n",
    "-- Simply add the tables you want to share to your SHARE:\n",
    "-- ALTER SHARE dbdemos_patient_readmission_visits ADD TABLE patients;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d88a6409-5076-48c0-9b46-94282b2feb12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE SHARE mcutini_diabetes_readmissions_share;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7326cd75-1d71-4638-a91f-34a0e7250f06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 3 - Data Analysis\n",
    "\n",
    "### Cells 29-35 show the experience of analyzing your data using Python and SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32bb7610-bfa3-4cfa-bab6-7a832e9d06ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "select * from patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df199ad6-ab52-44a0-8230-ef4d3d16caf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select GENDER, ceil(months_between(current_date(),BIRTHDATE)/12/5)*5 as age, count(*) as count from patients group by GENDER, age order by age\n",
    "-- Can use buildin visualization (Area: Key: age, group: gender_source_value, Values: count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05883400-f46b-4043-ac42-fded8cca6334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "px.area(_sqldf.toPandas(), x=\"age\", y=\"count\", color=\"GENDER\", line_group=\"GENDER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e282ce5-ae36-4a4d-a4aa-988157761c6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We can also leverage pure Python to access data\n",
    "from pyspark.sql.functions import col, desc\n",
    "df = spark.table(\"patients\").join(spark.table(\"conditions\"), col(\"Id\")==col(\"PATIENT\")) \\\n",
    "          .groupBy(['GENDER', 'conditions.DESCRIPTION']).count() \\\n",
    "          .orderBy(desc('count')).limit(20).toPandas()\n",
    "#And use our usual plot libraries\n",
    "px.bar(df, x=\"DESCRIPTION\", y=\"count\", color=\"GENDER\", barmode=\"group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db78218d-8588-4593-ab05-ba029447f79b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cohort Definition\n",
    "\n",
    "Let's define a cohort that we can do analysis on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ebae259-855c-41e6-a639-17ef2a7d7235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE cohort (\n",
    "  id INT,\n",
    "  name STRING,\n",
    "  patient STRING,\n",
    "  cohort_start_date DATE,\n",
    "  cohort_end_date DATE\n",
    ");\n",
    "ALTER TABLE cohort OWNER TO `account users`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0823d19-7d5b-4800-a4d0-75f8f75d8991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.sql.functions import lit\n",
    "def create_save_cohort(name, condition_codes = []):\n",
    "  cohort1 = (spark.sql('select patient, to_date(start) as cohort_start_date, to_date(stop) as cohort_end_date from conditions')\n",
    "                 .withColumn('id', lit(random.randint(999999, 99999999)))\n",
    "                 .withColumn('name', lit(name)))\n",
    "  if len(condition_codes)> 0:\n",
    "    cohort1 = cohort1.where(col('CODE').isin(condition_codes))\n",
    "  cohort1.write.mode(\"append\").saveAsTable('cohort')\n",
    "\n",
    "#Create cohorts based on patient condition (for ex: 840539006 is COVID)\n",
    "create_save_cohort('COVID-19-cohort', [840539006])\n",
    "create_save_cohort('heart-condition-cohort', [1505002, 32485007, 305351004, 76464004])\n",
    "create_save_cohort('all_patients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94e435b7-d563-4e9e-9050-a5221495f6bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 4 - Data Science and Machine Learning\n",
    "\n",
    "### Cells 38-50 show how do to data science on your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2376d533-cd05-456f-89ef-c39171bd9c7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/main/images/hls/patient-readmission/hls-patient-readmision-flow-4.png\" style=\"float: right; margin-left: 30px; margin-top:10px\" width=\"650px\" />\n",
    "\n",
    "We have cleaned and secured our data. We have also created cohorts of patients to analyze. We can now predict 30-day readmissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "796cab6a-43f0-4b2e-8bfa-b16aa5dae6db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow==2.19.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a6b128f-76bd-4d73-a3e3-340c2ed6c62a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80046596-eadf-4752-8af3-b4e2d2456167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "# Let's create our label: we'll predict the  30 days readmission risk\n",
    "windowSpec = Window.partitionBy(\"PATIENT\").orderBy(\"START\")\n",
    "labels = spark.table('encounters').select(\"PATIENT\", \"Id\", \"START\", \"STOP\") \\\n",
    "              .withColumn('30_DAY_READMISSION', F.when(F.col('START').cast('long') - F.lag(F.col('STOP')).over(windowSpec).cast('long') < 30*24*60*60, 1).otherwise(0))\n",
    "display(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b32bfe3f-6438-466f-8dc8-1f39a9b6fd29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "# Define Patient Features logic\n",
    "def compute_pat_features(data):\n",
    "  data = data.pandas_api()\n",
    "  data = ps.get_dummies(data, columns=['MARITAL', 'RACE', 'ETHNICITY', 'GENDER'],dtype = 'int64').to_spark()\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c23bdf78-ce25-4f7c-9cb8-65eb6f14e459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cohort_name = 'all_patients' #or could be 'COVID-19-cohort'\n",
    "cohort = spark.sql(f\"SELECT p.* FROM cohort c INNER JOIN patients p on c.patient=p.id WHERE c.name='{cohort_name}'\").dropDuplicates([\"id\"])\n",
    "cohort_features_df = compute_pat_features(cohort)\n",
    "cohort_features_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa43bce-fa00-4502-8472-0e57963892e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def compute_enc_features(data):\n",
    "  data = data.dropDuplicates([\"Id\"])\n",
    "  data = data.withColumn('enc_length', F.unix_timestamp(col('stop'))- F.unix_timestamp(col('start')))\n",
    "  data = data.pandas_api()\n",
    "#   return data\n",
    "  data = ps.get_dummies(data, columns=['ENCOUNTERCLASS'],dtype = 'int64').to_spark()\n",
    "  \n",
    "  return (\n",
    "    data\n",
    "    .select(\n",
    "      col('Id').alias('ENCOUNTER_ID'),\n",
    "      'BASE_ENCOUNTER_COST',\n",
    "      'TOTAL_CLAIM_COST',\n",
    "      'PAYER_COVERAGE',\n",
    "      'enc_length',\n",
    "      'ENCOUNTERCLASS_ambulatory',\n",
    "      'ENCOUNTERCLASS_emergency',\n",
    "      'ENCOUNTERCLASS_hospice',\n",
    "      'ENCOUNTERCLASS_inpatient',\n",
    "      'ENCOUNTERCLASS_outpatient',\n",
    "      'ENCOUNTERCLASS_wellness',\n",
    "    )\n",
    "  )\n",
    "\n",
    "enc_features_df = compute_enc_features(spark.table('encounters'))\n",
    "display(enc_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fb2e23a-7471-4179-8a77-a2501f7abde7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "enc_features_df = compute_enc_features(spark.table('encounters'))\n",
    "training_dataset = cohort_features_df.join(labels, [labels.PATIENT==cohort_features_df.Id], \"inner\") \\\n",
    "                                     .join(enc_features_df, [labels.Id==enc_features_df.ENCOUNTER_ID], \"inner\") \\\n",
    "                                     .drop(\"Id\", \"_rescued_data\", \"SSN\", \"DRIVERS\", \"PASSPORT\", \"FIRST\", \"LAST\", \"ADDRESS\", \"BIRTHPLACE\")\n",
    "### Adding extra feature such as patient age at encounter\n",
    "training_dataset = training_dataset.withColumnRenamed(\"PATIENT\", \"patient_id\") \\\n",
    "                                   .withColumn(\"age_at_encounter\", ((F.datediff(col('START'), col('BIRTHDATE'))) / 365.25))\n",
    "\n",
    "training_dataset.write.mode('overwrite').saveAsTable(\"training_dataset\")\n",
    "display(spark.table(\"training_dataset\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ff07d4f-fd29-4db8-afe4-b8ffe86b3c0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89f2b4ed-873a-4ea0-97a6-69b382d8143d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_names = ['MARITAL_M', 'MARITAL_S', 'RACE_asian', 'RACE_black', 'RACE_hawaiian', 'RACE_other', 'RACE_white', 'ETHNICITY_hispanic', 'ETHNICITY_nonhispanic', 'GENDER_F', 'GENDER_M', 'INCOME'] \\\n",
    "              + ['BASE_ENCOUNTER_COST', 'TOTAL_CLAIM_COST', 'PAYER_COVERAGE', 'enc_length', 'ENCOUNTERCLASS_ambulatory', 'ENCOUNTERCLASS_emergency', 'ENCOUNTERCLASS_hospice', 'ENCOUNTERCLASS_inpatient', 'ENCOUNTERCLASS_outpatient', 'ENCOUNTERCLASS_wellness'] \\\n",
    "              + ['age_at_encounter'] \\\n",
    "              + ['30_DAY_READMISSION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9222038f-d144-4179-830f-a10f1177b962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from datetime import datetime\n",
    "model_name = \"mcutini_diabetes_readmissions\"\n",
    "xp_path = \"/Shared/dbdemos/experiments/lakehouse-patient-admission\"\n",
    "xp_name = f\"automl_churn_{datetime.now().strftime('%Y-%m-%d_%H:%M:%S')}\"\n",
    "try:\n",
    "    from databricks import automl\n",
    "    automl_run = automl.classify(\n",
    "        experiment_name = xp_name,\n",
    "        experiment_dir = xp_path,\n",
    "        dataset = training_dataset.select(feature_names),\n",
    "        target_col = \"30_DAY_READMISSION\",\n",
    "        primary_metric=\"roc_auc\",\n",
    "        timeout_minutes = 10\n",
    "    )\n",
    "    #Make sure all users can access dbdemos shared experiment\n",
    "    DBDemos.set_experiment_permission(f\"{xp_path}/{xp_name}\")\n",
    "except Exception as e:\n",
    "    if \"cannot import name 'automl'\" in str(e):\n",
    "        # Note: cannot import name 'automl' from 'databricks' likely means you're using serverless. Dbdemos doesn't support autoML serverless API - this will be improved soon.\n",
    "        # Adding a temporary workaround to make sure it works well for now - ignore this for classic run\n",
    "        automl_run = DBDemos.create_mockup_automl_run(f\"{xp_path}/{xp_name}\", training_dataset.select(feature_names).toPandas(), model_name = model_name, target_col = \"30_DAY_READMISSION\")\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15e4387b-45b0-4400-88c3-50465f49d6f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3878688999324095,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Diabetes Readmissions Demo",
   "widgets": {
    "catalog_name": {
     "currentValue": "mcutini",
     "nuid": "dd328b9d-cbac-4d61-b8e7-4e52ebb02329",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Catalog Name",
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Catalog Name",
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "destination": {
     "currentValue": "./output/",
     "nuid": "48c06932-a6ee-4864-80c5-c6287e215bac",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "./output/",
      "label": "Base Directory",
      "name": "destination",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "./output/",
      "label": "Base Directory",
      "name": "destination",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "diabetes_readmissions",
     "nuid": "834b4f2b-bf41-4505-9a38-69212fcd8bcf",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Schema Name",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Schema Name",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
